{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: Train a Quadcopter How to Fly\n",
    "\n",
    "Design an agent that can fly a quadcopter, and then train it using a reinforcement learning algorithm of your choice! Try to apply the techniques you have learnt, but also feel free to come up with innovative ideas and test them.\n",
    "\n",
    "![Quadcopter doing a flip trying to takeoff from the ground](images/quadcopter_tumble.png)\n",
    "\n",
    "## Instructions\n",
    "\n",
    "> **Note**: If you haven't done so already, follow the steps in this repo's README to install ROS, and ensure that the simulator is running and correctly connecting to ROS.\n",
    "\n",
    "When you are ready to start coding, take a look at the `quad_controller_rl/src/` (source) directory to better understand the structure. Here are some of the salient items:\n",
    "\n",
    "- `src/`: Contains all the source code for the project.\n",
    "  - `quad_controller_rl/`: This is the root of the Python package you'll be working in.\n",
    "  - ...\n",
    "  - `tasks/`: Define your tasks (environments) in this sub-directory.\n",
    "    - `__init__.py`: When you define a new task, you'll have to import it here.\n",
    "    - `base_task.py`: Generic base class for all tasks, with documentation.\n",
    "    - `takeoff.py`: This is the first task, already defined for you, and set to run by default.\n",
    "  - ...\n",
    "  - `agents/`: Develop your reinforcement learning agents here.\n",
    "    - `__init__.py`: When you define a new agent, you'll have to import it here, just like tasks.\n",
    "    - `base_agent.py`: Generic base class for all agents, with documentation.\n",
    "    - `policy_search.py`: A sample agent has been provided here, and is set to run by default.\n",
    "  - ...\n",
    "\n",
    "### Tasks\n",
    "\n",
    "Open up the base class for tasks, `BaseTask`, defined in `tasks/base_task.py`:\n",
    "\n",
    "```python\n",
    "class BaseTask:\n",
    "    \"\"\"Generic base class for reinforcement learning tasks.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Define state and action spaces, initialize other task parameters.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def set_agent(self, agent):\n",
    "        \"\"\"Set an agent to carry out this task; to be called from update.\"\"\"\n",
    "        self.agent = agent\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset task and return initial condition.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def update(self, timestamp, pose, angular_velocity, linear_acceleration):\n",
    "        \"\"\"Process current data, call agent, return action and done flag.\"\"\"\n",
    "        raise NotImplementedError            \n",
    "```\n",
    "\n",
    "All tasks must inherit from this class to function properly. You will need to override the `reset()` and `update()` methods when defining a task, otherwise you will get `NotImplementedError`'s. Besides these two, you should define the state (observation) space and the action space for the task in the constructor, `__init__()`, and initialize any other variables you may need to run the task.\n",
    "\n",
    "Now compare this with the first concrete task `Takeoff`, defined in `tasks/takeoff.py`:\n",
    "\n",
    "```python\n",
    "class Takeoff(BaseTask):\n",
    "    \"\"\"Simple task where the goal is to lift off the ground and reach a target height.\"\"\"\n",
    "    ...\n",
    "```\n",
    "\n",
    "In `__init__()`, notice how the state and action spaces are defined using [OpenAI Gym spaces](https://gym.openai.com/docs/#spaces), like [`Box`](https://github.com/openai/gym/blob/master/gym/spaces/box.py). These objects provide a clean and powerful interface for agents to explore. For instance, they can inspect the dimensionality of a space (`shape`), ask for the limits (`high` and `low`), or even sample a bunch of observations using the `sample()` method, before beginning to interact with the environment. We also set a time limit (`max_duration`) for each episode here, and the height (`target_z`) that the quadcopter needs to reach for a successful takeoff.\n",
    "\n",
    "The `reset()` method is meant to give you a chance to reset/initialize any variables you need in order to prepare for the next episode. You do not need to call it yourself; it will be invoked externally. And yes, it will be called once before each episode, including the very first one. Here `Takeoff` doesn't have any episode variables to initialize, but it must return a valid _initial condition_ for the task, which is a tuple consisting of a [`Pose`](http://docs.ros.org/api/geometry_msgs/html/msg/Pose.html) and [`Twist`](http://docs.ros.org/api/geometry_msgs/html/msg/Twist.html) object. These are ROS message types used to convey the pose (position, orientation) and velocity (linear, angular) you want the quadcopter to have at the beginning of an episode. You may choose to supply the same initial values every time, or change it a little bit, e.g. `Takeoff` drops the quadcopter off from a small height with a bit of randomness.\n",
    "\n",
    "> **Tip**: Slightly randomized initial conditions can help the agent explore the state space faster.\n",
    "\n",
    "Finally, the `update()` method is perhaps the most important. This is where you define the dynamics of the task and engage the agent. It is called by a ROS process periodically (roughly 30 times a second, by default), with current data from the simulation. A number of arguments are available: `timestamp` (you can use this to check for timeout, or compute velocities), `pose` (position, orientation of the quadcopter), `angular_velocity`, and `linear_acceleration`. You do not have to include all these variables in every task, e.g. `Takeoff` only uses pose information, and even that requires a 7-element state vector.\n",
    "\n",
    "Once you have prepared the state you want to pass on to your agent, you will need to compute the reward, and check whether the episode is complete (e.g. agent crossed the time limit, or reached a certain height). Note that these two things (`reward` and `done`) are based on actions that the agent took in the past. When you are writing your own agents, you have to be mindful of this.\n",
    "\n",
    "Now you can pass in the `state`, `reward` and `done` values to the agent's `step()` method and expect an action vector back that matches the action space that you have defined, in this case a `Box(6,)`. After checking that the action vector is non-empty, and clamping it to the space limits, you have to convert it into a ROS `Wrench` message. The first 3 elements of the action vector are interpreted as force in x, y, z directions, and the remaining 3 elements convey the torque to be applied around those axes, respectively.\n",
    "\n",
    "Return the `Wrench` object (or `None` if you don't want to take any action) and the `done` flag from your `update()` method (note that when `done` is `True`, the `Wrench` object is ignored, so you can return `None` instead). This will be passed back to the simulation as a control command, and will affect the quadcopter's pose, orientation, velocity, etc. You will be able to gauge the effect when the `update()` method is called in the next time step.\n",
    "\n",
    "### Agents\n",
    "\n",
    "Reinforcement learning agents are defined in a similar way. Open up the generic agent class, `BaseAgent`, defined in `agents/base_agent.py`, and the sample agent `RandomPolicySearch` defined in `agents/policy_search.py`. They are actually even simpler to define - you only need to implement the `step()` method that is discussed above. It needs to consume `state` (vector), `reward` (scalar value) and `done` (boolean), and produce an `action` (vector). The state and action vectors must match the respective space indicated by the task. And that's it!\n",
    "\n",
    "Well, that's just to get things working correctly! The sample agent given `RandomPolicySearch` uses a very simplistic linear policy to directly compute the action vector as a dot product of the state vector and a matrix of weights. Then, it randomly perturbs the parameters by adding some Gaussian noise, to produce a different policy. Based on the average reward obtained in each episode (\"score\"), it keeps track of the best set of parameters found so far, how the score is changing, and accordingly tweaks a scaling factor to widen or tighten the noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "raw_mimetype": "text/html"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"width: 100%; text-align: center;\">\n",
       "    <h3>Teach a Quadcopter How to Tumble</h3>\n",
       "    <video poster=\"images/quadcopter_tumble.png\" width=\"640\" controls muted>\n",
       "        <source src=\"images/quadcopter_tumble.mp4\" type=\"video/mp4\" />\n",
       "        <p>Video: Quadcopter tumbling, trying to get off the ground</p>\n",
       "    </video>\n",
       "</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<div style=\"width: 100%; text-align: center;\">\n",
    "    <h3>Teach a Quadcopter How to Tumble</h3>\n",
    "    <video poster=\"images/quadcopter_tumble.png\" width=\"640\" controls muted>\n",
    "        <source src=\"images/quadcopter_tumble.mp4\" type=\"video/mp4\" />\n",
    "        <p>Video: Quadcopter tumbling, trying to get off the ground</p>\n",
    "    </video>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, this agent performs very poorly on the task. It does manage to move the quadcopter, which is good, but instead of a stable takeoff, it often leads to dizzying cartwheels and somersaults! And that's where you come in - your first _task_ is to design a better agent for this takeoff task. Instead of messing with the sample agent, create new file in the `agents/` directory, say `policy_gradients.py`, and define your own agent in it. Remember to inherit from the base agent class, e.g.:\n",
    "\n",
    "```python\n",
    "class DDPG(BaseAgent):\n",
    "    ...\n",
    "```\n",
    "\n",
    "You can borrow whatever you need from the sample agent, including ideas on how you might modularize your code (using helper methods like `act()`, `learn()`, `reset_episode_vars()`, etc.).\n",
    "\n",
    "> **Note**: This setup may look similar to the common OpenAI Gym paradigm, but there is one small yet important difference. Instead of the agent calling a method on the environment (to execute an action and obtain the resulting state, reward and done value), here it is the task that is calling a method on the agent (`step()`). If you plan to store experience tuples for learning, you will need to cache the last state ($S_{t-1}$) and last action taken ($A_{t-1}$), then in the next time step when you get the new state ($S_t$) and reward ($R_t$), you can store them along with the `done` flag ($\\left\\langle S_{t-1}, A_{t-1}, R_t, S_t, \\mathrm{done?}\\right\\rangle$).\n",
    "\n",
    "When an episode ends, the agent receives one last call to the `step()` method with `done` set to `True` - this is your chance to perform any cleanup/reset/batch-learning (note that no reset method is called on an agent externally). The action returned on this last call is ignored, so you may safely return `None`. The next call would be the beginning of a new episode.\n",
    "\n",
    "One last thing - in order to run your agent, you will have to edit `agents/__init__.py` and import your agent class in it, e.g.:\n",
    "\n",
    "```python\n",
    "from quad_controller_rl.agents.policy_gradients import DDPG\n",
    "```\n",
    "\n",
    "Then, while launching ROS, you will need to specify this class name on the commandline/terminal:\n",
    "\n",
    "```bash\n",
    "roslaunch quad_controller_rl rl_controller.launch agent:=DDPG\n",
    "```\n",
    "\n",
    "Okay, now the first task is cut out for you - follow the instructions below to implement an agent that learns to take off from the ground. For the remaining tasks, you get to define the tasks as well as the agents! Use the `Takeoff` task as a guide, and refer to the `BaseTask` docstrings for the different methods you need to override. Use some debug print statements to understand the flow of control better. And just like creating new agents, new tasks must inherit `BaseTask`, they need be imported into `tasks/__init__.py`, and specified on the commandline when running:\n",
    "\n",
    "```bash\n",
    "roslaunch quad_controller_rl rl_controller.launch task:=Hover agent:=DDPG\n",
    "```\n",
    "\n",
    "> **Tip**: You typically need to launch ROS and then run the simulator manually. But you can automate that process by either copying/symlinking your simulator to `quad_controller_rl/sim/DroneSim` (`DroneSim` must be an executable/link to one), or by specifying it on the command line, as follows:\n",
    "> \n",
    "> ```bash\n",
    "> roslaunch quad_controller_rl rl_controller.launch task:=Hover agent:=DDPG sim:=<full path>\n",
    "> ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Takeoff\n",
    "\n",
    "### Implement takeoff agent\n",
    "\n",
    "Train an agent to successfully lift off from the ground and reach a certain threshold height. Develop your agent in a file under `agents/` as described above, implementing at least the `step()` method, and any other supporting methods that might be necessary. You may use any reinforcement learning algorithm of your choice (note that the action space consists of continuous variables, so that may somewhat limit your choices).\n",
    "\n",
    "The task has already been defined (in `tasks/takeoff.py`), which you should not edit. The default target height (Z-axis value) to reach is 10 units above the ground. And the reward function is essentially the negative absolute distance from that set point (upto some threshold). An episode ends when the quadcopter reaches the target height (x and y values, orientation, velocity, etc. are ignored), or when the maximum duration is crossed (5 seconds).  See `Takeoff.update()` for more details, including episode bonus/penalty.\n",
    "\n",
    "As you develop your agent, it's important to keep an eye on how it's performing. Build in a mechanism to log/save the total rewards obtained in each episode to file. Once you are satisfied with your agent's performance, return to this notebook to plot episode rewards, and answer the questions below.\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "Plot the total rewards obtained in each episode, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "import pandas\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning(title, file_name):\n",
    "    csv = pandas.read_csv(file_name)\n",
    "\n",
    "    rewards = csv[['total_reward']].values\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    vals = csv.values\n",
    "    episodes = [i for i in range(1, len(rewards))]\n",
    "\n",
    "    ax.plot(rewards)\n",
    "    ax.set_ylabel(\"Reward\")\n",
    "    ax.set_xlabel(\"Episodes.\\n\\nExploration phase stops after 200 turns\")\n",
    "    ax.set_title(title)\n",
    "\n",
    "plot_learning(\"Takeoff - Task\", \"../out/takeoff.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q**: What algorithm did you use? Briefly discuss why you chose it for this task.\n",
    "\n",
    "**A**: \n",
    "\n",
    "\n",
    "**Q**: Using the episode rewards plot, discuss how the agent learned over time.\n",
    "\n",
    "- Was it an easy task to learn or hard?\n",
    "- Was there a gradual learning curve, or an aha moment?\n",
    "- How good was the final performance of the agent? (e.g. mean rewards over the last 10 episodes)\n",
    "\n",
    "**A**:\n",
    "\n",
    "\n",
    "## Task 2: Hover\n",
    "\n",
    "### Implement hover agent\n",
    "\n",
    "Now, your agent must take off and hover at the specified set point (say, 10 units above the ground). Same as before, you will need to create an agent and implement the `step()` method (and any other supporting methods) to apply your reinforcement learning algorithm. You may use the same agent as before, if you think your implementation is robust, and try to train it on the new task. But then remember to store your previous model weights/parameters, in case your results were worth keeping.\n",
    "\n",
    "### States and rewards\n",
    "\n",
    "Even if you can use the same agent, you will need to create a new task, which will allow you to change the state representation you pass in, how you verify when the episode has ended (the quadcopter needs to hover for at least a few seconds), etc. In this hover task, you may want to pass in the target height as part of the state (otherwise how would the agent know where you want it to go?). You may also need to revisit how rewards are computed. You can do all this in a new task file, e.g. `tasks/hover.py` (remember to follow the steps outlined above to create a new task):\n",
    "\n",
    "```python\n",
    "class Hover(BaseTask):\n",
    "    ...\n",
    "```\n",
    "\n",
    "**Q**: Did you change the state representation or reward function? If so, please explain below what worked best for you, and why you chose that scheme. Include short code snippet(s) if needed.\n",
    "\n",
    "**A**: \n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "**Q**: Discuss your implementation below briefly, using the following questions as a guide:\n",
    "\n",
    "- What algorithm(s) did you try? What worked best for you?\n",
    "- What was your final choice of hyperparameters (such as $\\alpha$, $\\gamma$, $\\epsilon$, etc.)?\n",
    "- What neural network architecture did you use (if any)? Specify layers, sizes, activation functions, etc.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs. Comment on any changes in learning behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "plot_learning(\"Hovering Task\", \"../out/hover.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Landing\n",
    "\n",
    "What goes up, must come down! But safely!\n",
    "\n",
    "### Implement landing agent\n",
    "\n",
    "This time, you will need to edit the starting state of the quadcopter to place it at a position above the ground (at least 10 units). And change the reward function to make the agent learn to settle down _gently_. Again, create a new task for this (e.g. `Landing` in `tasks/landing.py`), and implement the changes. Note that you will have to modify the `reset()` method to return a position in the air, perhaps with some upward velocity to mimic a recent takeoff.\n",
    "\n",
    "Once you're satisfied with your task definition, create another agent or repurpose an existing one to learn this task. This might be a good chance to try out a different approach or algorithm.\n",
    "\n",
    "### Initial condition, states and rewards\n",
    "\n",
    "**Q**: How did you change the initial condition (starting state), state representation and/or reward function? Please explain below what worked best for you, and why you chose that scheme. Were you able to build in a reward mechanism for landing gently?\n",
    "\n",
    "**A**: \n",
    "\n",
    "### Implementation notes\n",
    "\n",
    "**Q**: Discuss your implementation below briefly, using the same questions as before to guide you.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs. This task is a little different from the previous ones, since you're starting in the air. Was it harder to learn? Why/why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "\n",
    "plot_learning(\"Landing task\", \"../out/land.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Combined\n",
    "\n",
    "In order to design a complete flying system, you will need to incorporate all these basic behaviors into a single agent.\n",
    "\n",
    "### Setup end-to-end task\n",
    "\n",
    "The end-to-end task we are considering here is simply to takeoff, hover in-place for some duration, and then land. Time to create another task! But think about how you might go about it. Should it be one meta-task that activates appropriate sub-tasks, one at a time? Or would a single combined task with something like waypoints be easier to implement? There is no right or wrong way here - experiment and find out what works best (and then come back to answer the following).\n",
    "\n",
    "**Q**: What setup did you ultimately go with for this combined task? Explain briefly.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Implement combined agent\n",
    "\n",
    "Using your end-to-end task, implement the combined agent so that it learns to takeoff (at least 10 units above ground), hover (again, at least 10 units above ground), and gently come back to ground level.\n",
    "\n",
    "### Combination scheme and implementation notes\n",
    "\n",
    "Just like the task itself, it's up to you whether you want to train three separate (sub-)agents, or a single agent for the complete end-to-end task.\n",
    "\n",
    "**Q**: What did you end up doing? What challenges did you face, and how did you resolve them? Discuss any other implementation notes below.\n",
    "\n",
    "**A**:\n",
    "\n",
    "### Plot episode rewards\n",
    "\n",
    "As before, plot the episode rewards, either from a single run, or averaged over multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Read and plot episode rewards\n",
    "plot_learning(\"Combined Task\", \"../out/combined.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reflections\n",
    "\n",
    "**Q**: Briefly summarize your experience working on this project. You can use the following prompts for ideas.\n",
    "\n",
    "- What was the hardest part of the project? (e.g. getting started, running ROS, plotting, specific task, etc.)\n",
    "- How did you approach each task and choose an appropriate algorithm/implementation for it?\n",
    "- Did you find anything interesting in how the quadcopter or your agent behaved?\n",
    "\n",
    "**A**:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
